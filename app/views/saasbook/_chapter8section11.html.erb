<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/part2/chapter8/sub-chapter11.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="fallacies-and-pitfalls">
<h1><span class="section-number">8.11. </span>Fallacies and Pitfalls<a class="headerlink" href="#fallacies-and-pitfalls" title="Permalink to this headline">¶</a></h1>
<p><strong>Fallacy: 100% test coverage with all tests passing means no bugs.</strong></p>
<p>There are many reasons this statement can be false. Complete test coverage says
nothing about the quality of the individual tests. As well, some bugs may require
passing a certain value as a method argument (for example, to trigger a divide-by-zero
error), and control flow testing often cannot reveal such a bug. There may be bugs
in the interaction between your app and an external service such as TMDb; stubbing
out the service so you can perform local testing might mask such bugs.</p>
<p><strong>Pitfall: Dogmatically insisting on 100% test coverage all passing (green) before you ship.</strong></p>
<p>As we saw above, 100% test coverage is not only difficult to achieve at levels higher than C1,
but gives no guarantees of bug-freedom even if you do achieve it. Test coverage is a useful
tool for estimating the overall comprehensiveness of your test suite, but high confidence
requires a variety of testing methods—integration as well as unit, fuzzing as well as
hand-constructing test cases, define-use coverage as well as control-flow coverage, mutation
testing to expose additional holes in the test strategy, and so on. Indeed, in Chapter 12
we will discuss operational issues such as security and performance, which call for additional
testing strategies beyond the correctness-oriented ones described in this chapter.</p>
<p><strong>Fallacy: You don’t need much test code to be confident in the application.</strong></p>
<p>While insisting on 100% coverage may be counterproductive, so is going to the other extreme.
The <em>code-to-test ratio</em> in production systems (lines of noncomment code divided by lines of
tests of all types) is usually less than 1, that is, there are more lines of test than lines
of app code. As an extreme example, the SQLite database included with Rails contains over
1200 times as much test code as application code because of the wide variety of ways in
which it can be used and the wide variety of different kinds of systems on which it must
work properly! While there is controversy over how useful a measure the code-to-test ratio
is, given the high productivity of Ruby and its superior facilities for DRYing out your
test code, a <code class="code docutils literal notranslate"><span class="pre">rake</span> <span class="pre">stats</span></code> ratio between 0.2 and 0.5 is a reasonable target.</p>
<p><strong>Pitfall: Relying too heavily on just one kind of test (unit, functional, integration).</strong></p>
<p>Unit and functional tests are useful for covering rare corner cases and code paths. They
also tell you how well-factored or modular your code is: a module or method that is easy
to test has well-circumscribed external dependencies, which in turn reinforces that it can
be well tested in isolation. On the other hand, because of that very isolation, even 100% unit
test coverage tells you nothing about interactions <em>among</em> classes or modules. That’s where
integration-level tests such as the Cucumber scenarios of Chapter 7 are useful. Such tests
touch only a tiny fraction of all possible application paths and exercise only a few behaviors
in each method, but they do test the interfaces and interactions among modules. One rule of
thumb used at Google and elsewhere (Whittaker et al. 2012) is “70–20–10”: 70% short and focused
unit tests, 20% functional tests that touch multiple classes, 10% full-stack or integration
tests. See Chapter 7 for the complementary pitfall of over-reliance on integration tests.</p>
<p><strong>Pitfall: Undertested integration points due to over-stubbing.</strong></p>
<p>Mocking and stubbing confer many benefits, but they can also hide potential problems at
integration points—places where one class or module interacts with another. Suppose <code class="code docutils literal notranslate"><span class="pre">Movie</span></code>
has some interactions with another class <code class="code docutils literal notranslate"><span class="pre">Moviegoer</span></code>, but for the purposes of unit testing <code class="code docutils literal notranslate"><span class="pre">Movie</span></code>,
all calls to <code class="code docutils literal notranslate"><span class="pre">Moviegoer</span></code> methods are stubbed out, and vice versa. Because stubs are written to
“fake” the behavior of the collaborating class(es), we no longer know if <code class="code docutils literal notranslate"><span class="pre">Movie</span></code> “knows how to
talk to” <code class="code docutils literal notranslate"><span class="pre">Moviegoer</span></code> correctly. Good coverage with functional and integration tests, which don’t
stub out all calls across class boundaries, avoids this pitfall.</p>
<p><strong>Pitfall: Writing tests after the code rather than before.</strong></p>
<p>Thinking about “the code we wish we had” from the perspective of a test for that code
tends to result in code that is testable. This seems like an obvious tautology until you
try writing the code first without testability in mind, only to discover that surprisingly
often you end up with mock trainwrecks (see next pitfall) when you do try to write the test.</p>
<p>In addition, in the traditional Waterfall lifecycle described in Chapter 1, testing comes after
code development, but with SaaS that can be in “public beta” for months, no one would suggest
that testing should only begin after the beta period. Writing the tests first, whether for
fixing bugs or creating new features, eliminates this pitfall.</p>
<p><strong>Pitfall: Mock Trainwrecks.</strong></p>
<p>Mocks exist to help isolate your tests from their collaborators, but what about the collaborators’
collaborators? Suppose our <code class="code docutils literal notranslate"><span class="pre">Movie</span></code> object has a <code class="code docutils literal notranslate"><span class="pre">pics</span></code> attribute that returns a list
of images associated with the movie, each of which is a Picture object that has a format
attribute. You’re trying to mock a Movie object for use in a test, but you realize that the
method to which you’re passing the Movie object is going to expect to call methods on its
pics, so you find yourself doing something like this:</p>
<div class="highlight-ruby notranslate"><div class="highlight"><pre><span></span><span class="n">movie</span> <span class="o">=</span> <span class="n">double</span><span class="p">(</span><span class="s1">&#39;Movie&#39;</span><span class="p">,</span> <span class="ss">:pics</span> <span class="o">=&gt;</span> <span class="o">[</span><span class="n">double</span><span class="p">(</span><span class="s1">&#39;Picture&#39;</span><span class="p">,</span> <span class="ss">:format</span> <span class="o">=&gt;</span> <span class="s1">&#39;gif&#39;</span><span class="p">)</span><span class="o">]</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="no">Movie</span><span class="o">.</span><span class="n">count_pics</span><span class="p">(</span><span class="n">movie</span><span class="p">))</span><span class="o">.</span><span class="n">to</span> <span class="n">eq</span> <span class="mi">1</span>
</pre></div>
</div>
<p>This is called a <em>mock trainwreck</em>, and it’s a sign that the method under test (<code class="code docutils literal notranslate"><span class="pre">count_pics</span></code>) has
excessive knowledge of the innards of a <code class="code docutils literal notranslate"><span class="pre">Picture</span></code>. In Chapters 9 and 11 we’ll encounter a set of
additional guidelines to help you detect and resolve such <strong>code smells</strong>.</p>
<p><strong>Pitfall: Inadvertently creating dependencies regarding the order in which specs are run, for
example by using</strong> <code class="code docutils literal notranslate"><span class="pre">before(:all)</span></code> <strong>.</strong></p>
<p>If you specify actions to be performed only once for a whole group of test cases, you may
introduce dependencies among those test cases without noticing. For example, if a <code class="code docutils literal notranslate"><span class="pre">before</span> <span class="pre">:all</span></code>
block sets a variable and test example A changes the variable’s value, test example B could
come to rely on that change if A is usually run before B. Then B’s behav- ior in the future
might suddenly be different if B is run first, which might happen because guard prioritizes
running tests related to recently-changed code. Therefore it’s best to use
<code class="code docutils literal notranslate"><span class="pre">before</span> <span class="pre">:each</span></code> and <code class="code docutils literal notranslate"><span class="pre">after</span> <span class="pre">:each</span></code> whenever possible.</p>
<p><strong>Pitfall: Forgetting to re-prep the test database when the schema changes.</strong></p>
<p>Remember that tests run against a separate copy of the database, not the database used
in development (Section 4.2). Therefore, whenever you modify the schema by applying a
migration, you must also run <code class="code docutils literal notranslate"><span class="pre">rake</span> <span class="pre">db:test:prepare</span></code> to apply those changes to the test
database; otherwise your tests may fail because the test code doesn’t match the schema.</p>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="<%= section_path(chapter_id: 8, section_id: 10) %>" title="previous page"><span class="section-number">8.10. </span>The Plan-And-Document Perspective on Testing</a>
    <a class='right-next' id="next-link" href="<%= section_path(chapter_id: 8, section_id: 12) %>" title="next page"><span class="section-number">8.12. </span>Concluding Remarks: TDD vs. Conventional Debugging</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By sme777<br/>
        
            &copy; Copyright 2021, Armando Fox and David Patterson.<br/>
      </p>
    </div>
  </footer>
</main>
