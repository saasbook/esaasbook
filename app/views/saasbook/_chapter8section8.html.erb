    
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="other-testing-approaches-and-terminology">
<h1><span class="section-number">8.8. </span>Other Testing Approaches and Terminology<a class="headerlink" href="#other-testing-approaches-and-terminology" title="Permalink to this headline">¶</a></h1>
<p>The field of software testing is as broad and long-lived as software engineering and has
its own literature. Its range of techniques includes formalisms for proving things about
coverage, empirical techniques for selecting which tests to create, and directed-random
testing. Depending on an organization’s “testing culture,” you may hear different terminology
than we’ve used in this chapter. Ammann and Offutt’s <em>Introduction to Software Testing</em>
(Ammann and Offutt 2008) is one of the best comprehensive references on the subject. Their
approach is to divide a piece of code into <strong>basic blocks</strong>, each of which executes from the
beginning to the end with no possibility of branching, and then join these basic blocks
into a graph in which conditionals in the code result in graph nodes with multiple out-edges.
We can then think of testing as “covering the graph”: each test case tracks which nodes in
the graph it visits, and the fraction of all nodes visited at the end of the test suite is
the test coverage. Ammann and Offutt go on to analyze various structural aspects of software
from which such graphs can be extracted, and present systematic automated techniques for
achieving and measuring coverage of those graphs.</p>
<p>One insight that emerges from this approach is that the levels of testing described in the
previous section refer to control flow coverage, since they are only concerned with whether
specific parts of the code are executed or not. Another important coverage criterion is
<em>define–use coverage or DU-coverage</em>: given a variable x in some program, if we consider
every place that x is assigned a value and every place that the value of x is used,
DU-coverage asks what fraction of all <em>pairs</em> of define and use sites are exercised by
a test suite. This condition is weaker than all-paths coverage but can find errors that
control-flow coverage alone would miss.</p>
<p>Another testing term distinguishes <strong>black-box tests</strong>, whose design is based solely on the
software’s external specifications, from <strong>white-box tests</strong> (also called <em>glass-box tests</em>),
whose design reflects knowledge about the software’s implementation that is not implied
by external specifications. For example, the external specification of a hash table might
just state that when we store a key/value pair and later read that key, we should get
back the stored value. A black-box test would specify a random set of key/value pairs to
test this behavior, whereas a white-box test might exploit knowledge about the hash function
to construct worst-case test data that results in many hash collisions. Similarly, white-box
tests might focus on boundary values—parameter values likely to exercise different parts of
the code.</p>
<p><strong>Mutation testing</strong>, invented by Ammann and Offutt, is a test-automation technique in which
small but syntactically legal changes are automatically made to the program’s source code,
such as replacing <code class="code docutils literal notranslate"><span class="pre">a+b</span></code> with <code class="code docutils literal notranslate"><span class="pre">a-b</span></code> or replacing <code class="code docutils literal notranslate"><span class="pre">if</span> <span class="pre">(c)</span></code> with <code class="code docutils literal notranslate"><span class="pre">if</span> <span class="pre">(!c)</span></code>. Most such changes should
cause at least one test to fail, so a mutation that causes <em>no</em> test to fail indicates either
a lack of test coverage or a very strange program.</p>
<p><strong>Fuzz testing</strong> consists of throwing random data at your application and seeing what breaks.
In 2014, Google engineers reported that over a 2-year period, fuzz testing had helped find
over 1,000 bugs in the open source video-processing utility <code class="code docutils literal notranslate"><span class="pre">ffmpeg</span></code>. Fuzz testing has been
particularly useful for finding security vulnerabilities that are missed by both manual code
inspection and formal analysis, including stack and buffer overflows and unchecked null
pointers. While such memory bugs do not arise in interpreted languages like Ruby and Python
or in type-safe and memory-safe compiled languages such as Rust, fuzz testing can still find
interesting bugs in SaaS apps. <em>Random or black box fuzzing</em> either generates completely random
data or randomly mutates valid input data, such as changing certain bytes of metadata in a
JPEG image to test the robustness of the image decoder. <em>Smart fuzzing</em> incorporates knowledge
about the app’s structure and possibly a way to specify how to construct “realistic but fake”
fuzz data. For example, smart-fuzzing SaaS might include randomizing the variables and values
occurring in form postings or URIs, or attempting various cross-site scripting or SQL injection
attacks, which we’ll discuss in Chapter 12. Finally, <em>white-box fuzzing</em> uses <strong>symbolic execution</strong>,
which simulates execution of a program observing the conditions under which each branch is taken
or not, then generates fuzzed inputs to exercise the branch paths not taken during the simulated
execution. White-box fuzzing requires no explicit knowledge of the app’s structure and can
theoretically provide C2 (all paths) coverage, but in practice the size of the search space
is huge, and white-box fuzzing relies on a diverse set of “seed inputs” to be effective. This
combination of formal analysis and random directed testing is representative of the current state
of the art in thorough software testing. For a short contemporary survey of fuzz testing, see
Godefroid’s article in Communications of the ACM (Godefroid 2020).</p>
<p><strong>Self-Check 8.8.1.</strong> <em>The Microsoft Zune music player had an infamous bug that caused all Zunes
to “lock up” on December 31, 2008. Later analysis showed that the bug would be triggered on
the last day of any leap year. What kinds of tests—black-box, glass-box, mutation, or
fuzz—would have been likely to catch this bug?</em></p>
<blockquote>
<div><p>A glass-box test for the special code paths used for leap years would have been effective.
Fuzz testing might have been effective: since the bug occurs roughly once in every 1460 days,
a few thousand fuzz tests would likely have found it.</p>
</div></blockquote>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="<%= section_path(chapter_id: 8, section_id: 7) %>" title="previous page"><span class="section-number">8.7. </span>Coverage Concepts and Types of Tests</a>
    <a class='right-next' id="next-link" href="<%= section_path(chapter_id: 8, section_id: 9) %>" title="next page"><span class="section-number">8.9. </span>CHIPS:The Acceptance Test/Unit Test Cycle</a>

        </div>
        
        </div>
    </div>
