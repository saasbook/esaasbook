<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/part2/chapter8/sub-chapter7.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="coverage-concepts-and-types-of-tests">
<h1><span class="section-number">8.7. </span>Coverage Concepts and Types of Tests<a class="headerlink" href="#coverage-concepts-and-types-of-tests" title="Permalink to this headline">¶</a></h1>
<p>How much testing is enough? A poor but unfortunately widely-given answer is “As much
as you can before the release deadline.” A very coarse-grained alternative is the
<em>code-to-test ratio</em>, the number of non-comment lines of code divided by number of
lines of tests of all types. In production systems, this ratio is usually less than 1,
that is, there are more lines of test than lines of app code. The command <code class="code docutils literal notranslate"><span class="pre">rake</span> <span class="pre">stats</span></code>
issued in the root directory of a Rails app computes this ratio based on the number of
lines of RSpec tests and Cucumber scenarios.</p>
<p>Another widely-used metric that is more conservative is “when the rate of new bug reports
falls below some threshold.” This formulation acknowledges that while code can never be
proven bug-free, bugs are getting harder to find. But a more precise way to approach the
question is to combine such metrics with <strong>code coverage</strong>. Since the goal of testing is to
exercise the subject code in at least the same ways it would be exercised in production,
what fraction of those possibilities is actually exercised by the test suite? Surprisingly,
measuring coverage is not as straightforward as you might suspect. Figure 8.12 shows a simple
fragment of code that we will use to illustrate the definitions of several commonly-used
coverage terms.</p>
<div class="highlight-ruby notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyClass</span>
    <span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">y</span> <span class="o">&amp;&amp;</span> <span class="n">z</span><span class="p">)</span> <span class="k">then</span> <span class="n">bar</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">end</span>
        <span class="k">else</span>
            <span class="n">bar</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">end</span>
    <span class="k">end</span>
    <span class="k">def</span> <span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">;</span> <span class="vi">@w</span> <span class="o">=</span> <span class="n">x</span> <span class="p">;</span> <span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<ul class="simple">
<li><p>S0 or Method coverage: Is every method executed at least once by the test suite? Satisfying S0 requires calling <code class="code docutils literal notranslate"><span class="pre">foo</span></code> and <code class="code docutils literal notranslate"><span class="pre">bar</span></code> at least once each.</p></li>
<li><p>S1 or Call coverage or Entry/Exit coverage: Has each method been called from every place it could be called? Satisfying S1 requires calling bar from both line 4 and line 6.</p></li>
<li><p>C0 or Statement coverage: Is every statement of the source code executed at least once by the test suite, counting both branches of a conditional as a single statement? In addition to calling <code class="code docutils literal notranslate"><span class="pre">bar</span></code>, satisfying C0 would require calling <code class="code docutils literal notranslate"><span class="pre">foo</span></code> at least once with <code class="code docutils literal notranslate"><span class="pre">x</span></code> true (otherwise the statement in line 4 will never be executed), and at least once with <code class="code docutils literal notranslate"><span class="pre">y</span></code> false.</p></li>
<li><p>C1 or Branch coverage: Has each branch been taken in each direction at least once? Satisfying C1 would require calling <code class="code docutils literal notranslate"><span class="pre">foo</span></code> with both false and true values of <code class="code docutils literal notranslate"><span class="pre">x</span></code> and with values of <code class="code docutils literal notranslate"><span class="pre">y</span></code> and <code class="code docutils literal notranslate"><span class="pre">z</span></code> such that <code class="code docutils literal notranslate"><span class="pre">y</span> <span class="pre">&amp;&amp;</span> <span class="pre">z</span></code> in line 4 evaluates once to true and once to false. A more stringent condition, <em>decision coverage</em>, requires that each <em>subexpression</em> that independently affects a conditional expression be evaluated to true and false. In this example, a test would additionally have to separately set <code class="code docutils literal notranslate"><span class="pre">y</span></code> and <code class="code docutils literal notranslate"><span class="pre">z</span></code> so that the condition <code class="code docutils literal notranslate"><span class="pre">y</span> <span class="pre">&amp;&amp;</span> <span class="pre">z</span></code> fails once for <code class="code docutils literal notranslate"><span class="pre">y</span></code> being false and once for <code class="code docutils literal notranslate"><span class="pre">z</span></code> being false.</p></li>
<li><p>C2 or Path coverage: Has every possible route through the code been executed? In this simple example, where <code class="code docutils literal notranslate"><span class="pre">x,y,z</span></code> are treated as booleans, there are 8 possible paths.</p></li>
<li><p>Modified Condition/Decision Coverage (MCDC) combines a subset of the above levels: Every point of entry and exit in the program has been invoked at least once, every decision in the program has taken all possible outcomes at least once, and each condi- tion in a decision has been shown to independently affect that decision’s outcome.</p></li>
</ul>
<p>Achieving C0 coverage is relatively straightforward, and a goal of 100% C0 coverage is not
unreasonable. Achieving C1 coverage is more difficult since test cases must be constructed
more carefully to ensure each branch is taken at least once in each direction. C2 coverage
is most difficult of all, and not all testing experts agree on the additional value of
achieving 100% path coverage. Therefore, code coverage statistics are most valuable to the
extent that they highlight undertested or untested parts of the code and show the overall
comprehensiveness of your test suite. The SimpleCov gem is easy to configure and measures
and displays the C0 and C1 coverage of your specs, allowing you to browse file-by-file to
see which lines of your app were exercised by your test suites. If you have multiple suites,
such as a set of Cucumber features as well as a set of specs, you must decide whether you need
to know only whether a particular line of your app is exercised by <em>some</em> test, which may be
either a Cucumber scenario or an RSpec example, or whether you need to know which type of
test exercised it. SimpleCov does the former by default, but its instructions tell you how
to do the latter.</p>
<p>This chapter, and the above discussion of coverage, have focused on unit tests. Chapter 7
explained how user stories could become automated acceptance tests; we can think of a Cucumber
scenario as both a <strong>system test</strong>, because it exercises code in many different parts of the
application in the same ways a user would, as well as an <strong>acceptance test</strong>, because a
properly-written scenario reflects and verifies the behavior the user said they wanted.
In SaaS, such tests may also be called <em>full-stack tests</em>, since a typical scenario exercises
every part of the app from the browser-based UI to the database. Unlike unit tets, system
tests rarely rely on test doubles to isolate behavior; on the contrary, the goal is to simulate
real users as closely as possible.</p>
<p>Any test that covers more than one method but is not a full-stack test is generically an
<strong>integration test</strong>. For example, an RSpec test of a controller action would probably stub out
calls to the database and bypass the routing mechanism, neither of which is central to testing
the controller action itself, but would probably include interactions with mechanisms such as
parsing form input, which clearly are outside the controller action.</p>
<p>System and integration tests are important, but insufficient. Their resolution is poor: if an
integration test fails, it is harder to pinpoint the cause since the test touches many parts
of the code. Especially for system tests, coverage also tends to be poor because even though
a single scenario touches many classes, it executes only a few code paths in each class. For
the same reason, system and integration tests also tend to take longer to run. On the other
hand, while unit tests run quickly and can isolate the subject code with great precision
(improving both coverage resolution and error localization), because they rely on fake objects
to isolate the subject code, they may mask problems that would only arise in integration tests.
In other words, high assurance requires both good coverage and a mix of all three kinds of
tests. Figure 8.13 summarizes the relative strengths and weaknesses of different types of tests.</p>
<p>We have focused on testing for correctness (“did you build the thing right,”) but in practice,
other flavors of tests are part of any comprehensive test suite:</p>
<ul class="simple">
<li><p>A <strong>smoke test</strong> consists of a minimal attempt to operate the software, to see whether anything is obviously wrong before running the rest of the test suite. For example, if a low-level coding error prevents a SaaS app from displaying its home page or accepting logins, there is no point in running further tests.</p></li>
<li><p><strong>Compatibility testing</strong> is less prominent in SaaS since the app developers control the server environment, but may still be important for testing the app’s UI in different browsers. For example, Sauce Labs supports running SaaS integration tests on a variety of browsers and operating systems to check correct client behavior, and even captures a screencast of each run so you can visually check behaviors such as whether the same fonts look good in different browsers.</p></li>
<li><p><strong>Regression testing</strong> ensures that previously-fixed bugs do not reappear. We return to regression tests in Section 10.6.</p></li>
<li><p><strong>Performance, stress,</strong> and w[Computer security]security testing are types of <strong>non-functional testing</strong> that ensure the software meets these operational criteria, which are particularly important for SaaS. We return to these in Chapter 12.</p></li>
<li><p><strong>Accessibility testing</strong> ensures that the software is usable by persons with disabilities. In SaaS, accessibility testing focuses primarily on the client-side user experience.</p></li>
</ul>
<p><strong>Self-Check 8.7.1.</strong> <em>Why does high test coverage not necessarily imply a well-tested
application?</em></p>
<blockquote>
<div><p>Coverage says nothing about the quality of the tests. However, low coverage certainly
implies a poorly-tested application.</p>
</div></blockquote>
<p><strong>Self-Check 8.7.2.</strong> <em>What is the difference between C0 code coverage and code-to-test
ratio?</em></p>
<blockquote>
<div><p>C0 coverage is a <em>dynamic</em> measurement of what fraction of all statements are executed by a
test suite. Code-to-test ratio is a <em>static</em> measurement comparing the total number of lines
of code to the total number of lines of tests.</p>
</div></blockquote>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="<%= section_path(chapter_id: 8, section_id: 6) %>" title="previous page"><span class="section-number">8.6. </span>Fixtures and Factories</a>
    <a class='right-next' id="next-link" href="<%= section_path(chapter_id: 8, section_id: 8) %>" title="next page"><span class="section-number">8.8. </span>Other Testing Approaches and Terminology</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By sme777<br/>
        
            &copy; Copyright 2021, Armando Fox and David Patterson.<br/>
      </p>
    </div>
  </footer>
</main>
